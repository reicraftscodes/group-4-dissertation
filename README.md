
# Multimodal Facial Emotion Recognition Using Deep Learning Models
This dissertation project is a requirement for the completion of the Master’s in Artificial Intelligence degree at the University of the West of England, Bristol (UWE).

The complete detailed documentation for this project can be found on [GitBook documentation](https://app.gitbook.com/o/mef1saoUznyWqCwZ2dJh/s/a2v4RsuXXAJZxUqoRJdu/)

## Project Overview
This documentation covers the benchmarking of deep learning models for multimodal facial emotion recognition using RGB and thermal images. The objective is to compare model performance across different architectures and input modalities to improve the accuracy and reliability of emotion recognition systems.


## Set Up
Before running the project, please make sure you have the following installed on your machine:
- Use an IDE of your choice, such as PyCharm or VS Code.
- Python 3.9+

Or you can run it through Google Colab

##  How to run the project
To start/run/compile the project please follow the steps below:

**Step 1: Clone the repository**
```bash
  git clone https://gitlab.uwe.ac.uk/lmr2-sanejo/group-4-dissertation
```
**Step 2: Running the project**

Check the **README** for how to run the model once you’ve checked out the branch and read further details in the [GitBook Documentation](https://app.gitbook.com/o/mef1saoUznyWqCwZ2dJh/s/a2v4RsuXXAJZxUqoRJdu/)

Branch name

- ```feature4/vit_model```: Vision Transformer implementations can be found on branch.
- ```Mamba``` : Vision Mamba implementations can be found on branch.
-  ```CNN```: Vision Mamba implementations can be found on branch.


## Authors
- Fiorella Scarpino (21010043)
- May Sanejo (15006280)
- Soumia Kouadri (24058628)
