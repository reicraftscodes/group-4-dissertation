{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3492f02d",
      "metadata": {
        "id": "3492f02d"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a05e91",
      "metadata": {
        "id": "50a05e91"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f08dfe4",
      "metadata": {},
      "source": [
        "### 1.1 Setup for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8735bd29",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb7284c",
      "metadata": {},
      "outputs": [],
      "source": [
        " cd /content/drive/MyDrive/Data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "248c4e23",
      "metadata": {},
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19969f74",
      "metadata": {},
      "source": [
        "### 1.2 Install Depedencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8586dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "#pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa7ebc",
      "metadata": {
        "id": "aefa7ebc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms,models\n",
        "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f7f641",
      "metadata": {},
      "source": [
        "### 1.2 Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a55e95",
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "CLASSES = 7\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2 # change based on specification of computer\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f042cc2b",
      "metadata": {
        "id": "f042cc2b"
      },
      "source": [
        "## 2. Load Data and Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fef938",
      "metadata": {},
      "outputs": [],
      "source": [
        "#directories\n",
        "rgbDir = ['RGB','RgbAug']\n",
        "thermalDir = ['Thermal','ThermalAug']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa2bf36",
      "metadata": {},
      "source": [
        "### 2.1 Single Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a80eed3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a80eed3",
        "outputId": "e7ff0846-f2e1-4c8a-8051-eb9d7d017a39"
      },
      "outputs": [],
      "source": [
        "def loadDataSingleModality(modalDir):\n",
        "    '''\n",
        "    Loads and processes data from a single modality (either RGB or Thermal).\n",
        "    Args:\n",
        "        modalDir (list): List containing two directory paths - first for raw images, second for augmented images.\n",
        "    Returns:\n",
        "        pairedFiles (list): List of all image file paths.\n",
        "        tensorLabels (torch.Tensor): Tensor of integer-encoded labels corresponding to the images.\n",
        "    '''\n",
        "\n",
        "    #get list of files in the directory and combine them\n",
        "    pairedFiles = []\n",
        "    for data in modalDir:\n",
        "        pairedFiles.extend(glob.glob(os.path.join(data, \"*\")))\n",
        "    pairedFiles = sorted(pairedFiles)\n",
        "\n",
        "    print(f'Total files: {len(pairedFiles)}')\n",
        "    \n",
        "\n",
        "    #get the labels from the filenames\n",
        "    SMLabels = [os.path.basename(f).split('_')[1] for f in pairedFiles] #1 = emotion label\n",
        "  \n",
        "    #convert labels to integer\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    intLabels = le.fit_transform(SMLabels)\n",
        "    #convert targets to tensor\n",
        "    tensorLabels = torch.as_tensor(intLabels)\n",
        "    #class names\n",
        "    classNames = le.classes_\n",
        "\n",
        "    print(f'Data arranged as: {tensorLabels}')\n",
        "    print(f'\\nSample output:\\nFile: {pairedFiles[0]}\\nLabel: {SMLabels[0]}\\n')  \n",
        "\n",
        "    return pairedFiles, tensorLabels, classNames\n",
        "\n",
        "\n",
        "## For single modality testing (uaing raw and augmented data together) \n",
        "\n",
        "#change this to test either thermal or rgb\n",
        "modalDir = rgbDir\n",
        "\n",
        "#Call function to load data\n",
        "pairedFiles, tensorLabels, classNames = loadDataSingleModality(modalDir)\n",
        "\n",
        "#for saving results later\n",
        "if modalDir == rgbDir:\n",
        "    savingLabel = \"RGB\"\n",
        "else:\n",
        "    savingLabel = \"Thermal\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef46b5a",
      "metadata": {},
      "source": [
        "### 2.2 Multi Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14ebc3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadDataMultiModality(rgbDir,thermalDir):\n",
        "    '''\n",
        "    Loads and processes data from both RGB and Thermal modalities.\n",
        "    Args:\n",
        "        rgbDir (list): List containing two directory paths for RGB images - first for raw images, second for augmented images.\n",
        "        thermalDir (list): List containing two directory paths for Thermal images - first for raw images, second for augmented images.\n",
        "    Returns:\n",
        "        pairedFiles (list): List of tuples, each containing a pair of (RGB image path, Thermal image path).\n",
        "        tensorLabels (torch.Tensor): Tensor of integer-encoded labels corresponding to the images.\n",
        "    '''\n",
        "\n",
        "\n",
        "    #get list of files in the directory and combine them\n",
        "    rgbFiles = []\n",
        "    thermalFiles = []\n",
        "    for data in rgbDir:\n",
        "        rgbFiles.extend(glob.glob(os.path.join(data, \"*\")))\n",
        "    for data in thermalDir:\n",
        "        thermalFiles.extend(glob.glob(os.path.join(data, \"*\")))\n",
        "\n",
        "    #pairs thermal and rgb files together\n",
        "    pairedFiles = list(zip(rgbFiles, thermalFiles))\n",
        "    print(f\"Total pairs: {len(pairedFiles)}\")\n",
        "\n",
        "    #get the labels from the filenames\n",
        "    SMLabels = [os.path.basename(f).split('_')[1] for f in thermalFiles] #1 = emotion label\n",
        " \n",
        "    #convert labels to integer\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    intLabels = le.fit_transform(SMLabels)\n",
        "    #convert targets to tensor\n",
        "    tensorLabels = torch.as_tensor(intLabels)\n",
        "    #class names\n",
        "    classNames = le.classes_\n",
        "\n",
        "    print(f'Data arranged as: {tensorLabels}')\n",
        "    print(f'\\nSample output:\\nFile: {pairedFiles[0]}\\nLabel: {SMLabels[0]}\\n')  \n",
        "\n",
        "    return pairedFiles, tensorLabels, classNames\n",
        "\n",
        "\n",
        "## For multi modality testing (using raw and augmented data together)\n",
        "\n",
        "#Call function to load data\n",
        "pairedFiles, tensorLabels, classNames = loadDataMultiModality(rgbDir,thermalDir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1eae48",
      "metadata": {},
      "source": [
        "### 2.3 Split Data\n",
        "- 80:20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a573e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    pairedFiles, tensorLabels,\n",
        "    test_size=0.2,\n",
        "    stratify=tensorLabels,\n",
        "    random_state=42\n",
        ")\n",
        "print(f'Train size: {len(X_train)}\\nTest size: {len(X_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67063f12",
      "metadata": {
        "id": "67063f12"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a72e60",
      "metadata": {
        "id": "40a72e60"
      },
      "source": [
        "## 3. DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb67f53",
      "metadata": {},
      "source": [
        "### 3.1 Single Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e424a8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleModalImageDataset(Dataset):\n",
        "    def __init__(self, pairedFiles, labels, transform=None):\n",
        "        #initialise\n",
        "        self.pairedFiles = pairedFiles\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        #return the length of the dataset\n",
        "        return len(self.pairedFiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #generates samples\n",
        "        imagePath = self.pairedFiles[idx]\n",
        "        image = Image.open(imagePath).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "    \n",
        "\n",
        "train_dataset = SingleModalImageDataset(X_train, y_train, transform=transform)\n",
        "test_dataset = SingleModalImageDataset(X_test, y_test, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2151af09",
      "metadata": {},
      "source": [
        "### 3.2 Multi Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1bab7c",
      "metadata": {
        "id": "1b1bab7c"
      },
      "outputs": [],
      "source": [
        "class MultiModalImageDataset(Dataset):\n",
        "    def __init__(self, pairedFiles, labels, transform=None):\n",
        "        #initialise\n",
        "        self.pairedFiles = pairedFiles\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        #return the length of the dataset\n",
        "        return len(self.pairedFiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #generates samples\n",
        "        rgbPath,thermalPath  = self.pairedFiles[idx]\n",
        "        rgb = Image.open(rgbPath).convert('RGB')\n",
        "        thermal = Image.open(thermalPath).convert('RGB') #try L greyscale\n",
        "\n",
        "        if self.transform:\n",
        "            rgb = self.transform(rgb)\n",
        "            thermal = self.transform(thermal)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return (rgb,thermal),label\n",
        "    \n",
        "train_dataset = MultiModalImageDataset(X_train, y_train, transform=transform)\n",
        "test_dataset = MultiModalImageDataset(X_test, y_test, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52bfef92",
      "metadata": {},
      "source": [
        "### 3.3 Train and Test Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb67bef",
      "metadata": {
        "id": "beb67bef"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CFtn9xCdIXUG",
      "metadata": {
        "id": "CFtn9xCdIXUG"
      },
      "source": [
        "## 4. ShuffleNet V.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c25d06",
      "metadata": {},
      "source": [
        "### 4.1 Single Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ad44c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#initialise model\n",
        "model = models.shufflenet_v2_x1_0(weights=ShuffleNet_V2_X1_0_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, CLASSES)\n",
        "model.to(device)\n",
        "\n",
        "#loss function and optimiser\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "#to collect data for plotting later\n",
        "trainLosses = []\n",
        "testAccuracies = []\n",
        "predictionsArr = []\n",
        "targetsArr = []\n",
        "\n",
        "##Training\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    runningTrain = 0\n",
        "    #progress bar\n",
        "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimiser.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        #batch loss calculation\n",
        "        runningTrain += loss.item() * x.size(0)\n",
        "    trainLoss = runningTrain / len(train_loader.dataset)\n",
        "\n",
        "    ##Evaluation\n",
        "    model.eval()\n",
        "    accPre = 0\n",
        "    totalDatapoints = 0\n",
        "    #progress bar\n",
        "    for input, target in tqdm(test_loader, desc=f\"Epoch {epoch+1} Evaluating\"):\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input)\n",
        "            #accuracy calculation\n",
        "            predicted = outputs.argmax(1)\n",
        "\n",
        "            #https://medium.com/@heyamit10/building-a-multiclass-classification-model-in-pytorch-a-detailed-practical-guide-b03fc93aa400\n",
        "            predictionsArr.extend(predicted.cpu().numpy())\n",
        "            targetsArr.extend(target.cpu().numpy())\n",
        "            \n",
        "            accPre += (predicted == target).sum().item()\n",
        "            totalDatapoints += target.size(0)\n",
        "    finalAcc = accPre / totalDatapoints\n",
        "\n",
        "    trainLosses.append(trainLoss)\n",
        "    testAccuracies.append(finalAcc)\n",
        "\n",
        "\n",
        "    #prints each epoch's results\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {trainLoss:.4f}, Test Accuracy = {finalAcc:.4f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1adef83a",
      "metadata": {},
      "source": [
        "### 4.2 Multi Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3387f7d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create multi-modal CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b6f1fc",
      "metadata": {},
      "source": [
        "## 5. Visualise and Save Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519ba2f2",
      "metadata": {},
      "source": [
        "### 5.1 Save Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abd2b50",
      "metadata": {},
      "outputs": [],
      "source": [
        "saveResults = \"trainingResults\"\n",
        "os.makedirs(saveResults, exist_ok=True)\n",
        "\n",
        "#save training loss and accuracy to CSV\n",
        "metrics = pd.DataFrame({'Epoch': range(1, len(trainLosses) + 1), 'Training Loss': trainLoss,'Test Accuracy': testAccuracies}) #create each row as epoch\n",
        "\n",
        "#saves based on modality used\n",
        "if savingLabel == \"RGB\":\n",
        "    metrics.to_csv(os.path.join(saveResults, \"metrics_RGB.csv\"), index=False)\n",
        "else:   \n",
        "    metrics.to_csv(os.path.join(saveResults, \"metrics_Thermal.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e403f5bb",
      "metadata": {},
      "source": [
        "### 5.2 Visualise Graphs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f460f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#loss curve\n",
        "plt.plot(range(1, len(trainLosses) + 1), trainLosses, marker='o')\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "#saves based on modality used\n",
        "if savingLabel == \"RGB\":\n",
        "    plt.savefig(os.path.join(saveResults, \"trainLoss_RGB.png\"))\n",
        "else:\n",
        "    plt.savefig(os.path.join(saveResults, \"trainLoss_Thermal.png\"))\n",
        "plt.show()\n",
        "\n",
        "#accuracy\n",
        "plt.plot(range(1, len(testAccuracies) + 1), testAccuracies, marker='o')\n",
        "plt.title(\"Test Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "#saves based on modality used\n",
        "if savingLabel == \"RGB\":\n",
        "    plt.savefig(os.path.join(saveResults, \"testAccuracy_RGB.png\"))  \n",
        "else:\n",
        "    plt.savefig(os.path.join(saveResults, \"testAccuracy_Thermal.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde93d84",
      "metadata": {},
      "outputs": [],
      "source": [
        "#confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(targetsArr, predictionsArr)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(targetsArr, predictionsArr))\n",
        "#saves based on modality used\n",
        "if savingLabel == \"RGB\":\n",
        "    with open(os.path.join(saveResults, \"classificationReport_RGB.txt\"), \"w\") as f:\n",
        "        f.write(classification_report(targetsArr, predictionsArr))\n",
        "else:\n",
        "    with open(os.path.join(saveResults, \"classificationReport_Thermal.txt\"), \"w\") as f:\n",
        "        f.write(classification_report(targetsArr, predictionsArr))\n",
        " \n",
        "#visualisation of confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=classNames, yticklabels=classNames)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "#saves based on modality used\n",
        "if savingLabel == \"RGB\":\n",
        "    plt.savefig(os.path.join(saveResults, \"confusionMatrix_RGB.png\"))\n",
        "else:\n",
        "    plt.savefig(os.path.join(saveResults, \"confusionMatrix_Thermal.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cef3dd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualising some images and their labels\n",
        "\n",
        "#true images and labels\n",
        "imagesPlot, labelsPlot = next(iter(train_loader))\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "for i in range(3*3):\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, i+1)\n",
        "    ax.imshow(np.transpose(torchvision.utils.make_grid(imagesPlot[i].cpu(), normalize=True, padding=1).numpy(), (1, 2, 0)))\n",
        "    ax.set_title(classNames[labelsPlot[i]])\n",
        "    ax.axis('off')\n",
        "    plt.suptitle('Sample Training Images with True Labels',fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "#predicted images and labels\n",
        "imagesPlotPred, labelsPlotPred = next(iter(test_loader))\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "for i in range(3*3):\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, i+1)\n",
        "    ax.imshow(np.transpose(torchvision.utils.make_grid(imagesPlotPred[i].cpu(), normalize=True, padding=1).numpy(), (1, 2, 0)))\n",
        "    \n",
        "    trueLabel = classNames[labelsPlotPred[i]]\n",
        "    predictedLabel = classNames[predicted[i]]\n",
        "    \n",
        "    ax.set_title(f'True: {trueLabel}\\nPredicted: {predictedLabel}')\n",
        "    ax.axis(\"off\")\n",
        "    plt.suptitle('Sample Testing Images with True and Predicted Labels',fontsize=14)\n",
        "plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_VFasJI71LEz"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
